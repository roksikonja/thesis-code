{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "import tensorflow as tf\n",
    "from graph_nets import utils_tf\n",
    "from lib.gns import obses_to_cgraphs\n",
    "from lib.data_utils import extract_target_windows\n",
    "import numpy as np\n",
    "\n",
    "from experience import load_experience\n",
    "from lib.action_space import is_do_nothing_action\n",
    "from lib.constants import Constants as Const\n",
    "from lib.data_utils import make_dir, env_pf\n",
    "from lib.gns import (\n",
    "    tf_batched_graph_dataset,\n",
    "    get_graph_feature_dimensions,\n",
    "    GraphNetworkSwitching,\n",
    ")\n",
    "from lib.run_utils import create_logger\n",
    "from lib.visualizer import Visualizer, pprint\n",
    "\n",
    "Visualizer()\n",
    "np.random.seed(0)\n",
    "\n",
    "# experience_dir = make_dir(os.path.join(Const.RESULTS_DIR, \"performance-aug\"))\n",
    "experience_dir = make_dir(os.path.join(Const.EXPERIENCE_DIR, \"data-aug\"))\n",
    "results_dir = make_dir(os.path.join(Const.RESULTS_DIR, \"binary-linear\"))\n",
    "\n",
    "agent_name = \"agent-mip\"\n",
    "case_name = \"l2rpn_2019_art\"\n",
    "env_dc = True\n",
    "verbose = False\n",
    "\n",
    "case_results_dir = make_dir(os.path.join(results_dir, f\"{case_name}-{env_pf(env_dc)}\"))\n",
    "create_logger(logger_name=f\"{case_name}-{env_pf(env_dc)}\", save_dir=case_results_dir)\n",
    "\n",
    "case, collector = load_experience(case_name, agent_name, experience_dir, env_dc=env_dc)\n",
    "obses, actions, rewards, dones = collector.aggregate_data()\n",
    "\n",
    "pprint(\"    - Number of chronics:\", dones.sum())\n",
    "pprint(\"    - Observations:\", len(obses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Parameters\n",
    "\"\"\"\n",
    "n_window_targets = 0\n",
    "n_window_history = 1\n",
    "n_batch = 32\n",
    "n_epochs = 20\n",
    "\n",
    "\"\"\"\n",
    "    Datasets\n",
    "\"\"\"\n",
    "\n",
    "labels = is_do_nothing_action(actions, case.env).astype(float)\n",
    "pprint(\"    - Labels:\", f\"{int(labels.sum())}/{labels.shape[0]}\", \"{:.2f} %\".format(100 * labels.mean()))\n",
    "\n",
    "mask_positive = extract_target_windows(labels, mask=~dones, n_window=n_window_targets)\n",
    "mask_negative = np.logical_and(np.random.binomial(1, 0.05, len(labels)), ~mask_positive)\n",
    "mask_targets = np.logical_or(mask_positive, mask_negative)\n",
    "\n",
    "pprint(\"    - Mask (0):\", mask_negative.sum(), \"{:.2f} %\".format(100 * mask_negative.sum() / mask_targets.sum()))\n",
    "pprint(\"    - Mask (1):\", mask_positive.sum(), \"{:.2f} %\".format(100 * mask_positive.sum() / mask_targets.sum()))\n",
    "pprint(\"    - Mask:\", mask_targets.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'obses_to_cgraphs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-868e54984bde>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mcgraphs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mobses_to_cgraphs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobses\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdones\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcase\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmask_targets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_window\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mn_window_history\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mgraph_dims\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_graph_feature_dimensions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcgraphs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcgraphs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mcgraph_dims\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mgraph_dims\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"n_nodes\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mcase\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_sub\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"n_edges\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;36m2\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mcase\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_line\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mpprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"    - Cgraphs:\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcgraphs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"globals\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'obses_to_cgraphs' is not defined"
     ]
    }
   ],
   "source": [
    "cgraphs = obses_to_cgraphs(obses, dones, case, mask=mask_targets, n_window=n_window_history)\n",
    "graph_dims = get_graph_feature_dimensions(cgraphs=cgraphs)\n",
    "cgraph_dims = {**graph_dims, \"n_nodes\": case.env.n_sub, \"n_edges\": 2 * case.env.n_line}\n",
    "\n",
    "pprint(\"    - Cgraphs:\", len(cgraphs[\"globals\"]))\n",
    "for field in cgraphs:\n",
    "    pprint(f\"        - {field}:\", cgraphs[field][0].shape)\n",
    "    \n",
    "for n_feat in cgraph_dims:\n",
    "    pprint(f\"        - {n_feat}:\", cgraph_dims[n_feat])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_dataset = tf_batched_graph_dataset(cgraphs, n_batch=n_batch, **graph_dims)\n",
    "label_dataset = tf.data.Dataset.from_tensor_slices(labels[mask_targets]).batch(n_batch)\n",
    "dataset = tf.data.Dataset.zip((graph_dataset, label_dataset))\n",
    "\n",
    "\"\"\"\n",
    "    Signatures\n",
    "\"\"\"\n",
    "\n",
    "graphs_sig = utils_tf.specs_from_graphs_tuple(\n",
    "    next(iter(graph_dataset)), dynamic_num_graphs=True\n",
    ")\n",
    "labels_sig = tf.TensorSpec(shape=[None], dtype=tf.dtypes.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Model\n",
    "\"\"\"\n",
    "tf.random.set_seed(0)\n",
    "model = GraphNetworkSwitching(\n",
    "    pos_class_weight=1 / labels.mean(),\n",
    "    n_hidden=(512, 512, 512, 512, 512),\n",
    "    graphs_signature=graphs_sig,\n",
    "    labels_signature=labels_sig,\n",
    "    **cgraph_dims,\n",
    ")\n",
    "\n",
    "model_dir = os.path.join(case_results_dir, \"model-10\")\n",
    "checkpoint_path = os.path.join(model_dir, \"ckpts\")\n",
    "\n",
    "ckpt = tf.train.Checkpoint(model=model, optimizer=model.optimizer)\n",
    "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=2)\n",
    "\n",
    "if ckpt_manager.latest_checkpoint:\n",
    "    ckpt.restore(ckpt_manager.latest_checkpoint)\n",
    "    pprint(f\"Restoring checkpoint from:\", ckpt_manager.latest_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Training\n",
    "\"\"\"\n",
    "import time\n",
    "n_epochs = 10\n",
    "\n",
    "# Epoch Metrics\n",
    "metric_recall_e = tf.keras.metrics.Recall()\n",
    "metric_precision_e = tf.keras.metrics.Precision()\n",
    "metric_accuracy_e = tf.keras.metrics.Accuracy()\n",
    "metric_bce_e = tf.keras.metrics.BinaryCrossentropy(from_logits=False)\n",
    "recall_e = []\n",
    "precision_e = []\n",
    "accuracy_e = []\n",
    "bce_e = []\n",
    "\n",
    "# Batch Metrics\n",
    "metric_accuracy_b = tf.metrics.binary_accuracy\n",
    "metric_bce_b = tf.metrics.binary_crossentropy\n",
    "losses = []\n",
    "accuracy = []\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    start = time.time()\n",
    "\n",
    "    # Reset epoch metrics\n",
    "    metric_recall_e.reset_states()\n",
    "    metric_precision_e.reset_states()\n",
    "    metric_accuracy_e.reset_states()\n",
    "    metric_bce_e.reset_states()\n",
    "\n",
    "    for batch, (graph_batch, label_batch) in enumerate(dataset):\n",
    "        (\n",
    "            output_graphs,\n",
    "            loss,\n",
    "            probabilities,\n",
    "            predicted_labels,\n",
    "            gradients,\n",
    "        ) = model.train_step(graph_batch, label_batch)\n",
    "\n",
    "        # Batch Metric\n",
    "        bce = metric_bce_b(label_batch, probabilities)  # Control\n",
    "        acc = metric_accuracy_b(\n",
    "            label_batch, tf.cast(predicted_labels, dtype=tf.float64)\n",
    "        )\n",
    "\n",
    "        # Epoch Metrics\n",
    "        metric_recall_e(label_batch, predicted_labels)\n",
    "        metric_precision_e(label_batch, predicted_labels)\n",
    "        metric_accuracy_e(label_batch, predicted_labels)\n",
    "        metric_bce_e(label_batch, probabilities)\n",
    "\n",
    "        losses.append(loss.numpy())\n",
    "        accuracy.append(acc.numpy())\n",
    "\n",
    "        if batch % 5 == 0:\n",
    "            pprint(\n",
    "                \"        - Batch/Epoch:\",\n",
    "                f\"{batch}/{epoch}\",\n",
    "                \"loss = {:.4f}\".format(loss.numpy()),\n",
    "                \"bce = {:.4f}\".format(bce.numpy()),\n",
    "                \"acc = {} %\".format(int(100 * acc.numpy())),\n",
    "            )\n",
    "\n",
    "    recall_e.append(metric_recall_e.result().numpy())\n",
    "    precision_e.append(metric_precision_e.result().numpy())\n",
    "    accuracy_e.append(metric_accuracy_e.result().numpy())\n",
    "    bce_e.append(metric_bce_e.result().numpy())\n",
    "\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        ckpt_save_path = ckpt_manager.save()\n",
    "        pprint(f\"            - Saving checkpoint to:\", ckpt_save_path)\n",
    "        pprint(f\"            - Time taken for epoch:\", f\"{time.time() - start} secs\")\n",
    "\n",
    "ckpt_save_path = ckpt_manager.save()\n",
    "pprint(f\"    - Saving checkpoint to:\", ckpt_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_variables(model.trainable_variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from lib.tf_utils import print_variables\n",
    "import numpy as np\n",
    "\n",
    "losses = np.array(losses)\n",
    "accuracy = np.array(accuracy)\n",
    "accuracy_e = np.array(accuracy_e)\n",
    "bce_e = np.array(bce_e)\n",
    "recall_e = np.array(recall_e)\n",
    "precision_e = np.array(precision_e)\n",
    "\n",
    "make_dir(model_dir)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "    Batch Metrics\n",
    "\"\"\"\n",
    "fig, ax = plt.subplots(figsize=(Const.FIG_SIZE))\n",
    "ax.plot(losses, lw=1.0)\n",
    "ax.set_title(\"Batch training loss\")\n",
    "ax.set_xlabel(\"Batch\")\n",
    "ax.set_ylabel(\"Loss\")\n",
    "fig.savefig(os.path.join(model_dir, \"training-loss\"))\n",
    "# plt.close(fig)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(Const.FIG_SIZE))\n",
    "ax.plot(accuracy * 100, lw=1.0)\n",
    "ax.set_title(\"Batch accuracy\")\n",
    "ax.set_xlabel(\"Batch\")\n",
    "ax.set_ylabel(\"Accuracy [\\%]\")\n",
    "# ax.set_ylim([0, 100])\n",
    "fig.savefig(os.path.join(model_dir, \"training-accuracy\"))\n",
    "# plt.close(fig)\n",
    "\n",
    "\"\"\"\n",
    "    Epoch Metrics\n",
    "\"\"\"\n",
    "fig, ax = plt.subplots(figsize=(Const.FIG_SIZE))\n",
    "ax.plot(accuracy_e * 100, lw=1.0)\n",
    "ax.set_title(\"Epoch accuracy\")\n",
    "ax.set_xlabel(\"Epoch\")\n",
    "ax.set_ylabel(\"Accuracy [\\%]\")\n",
    "# ax.set_ylim([0, 100])\n",
    "fig.savefig(os.path.join(model_dir, \"epoch-accuracy\"))\n",
    "# plt.close(fig)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(Const.FIG_SIZE))\n",
    "ax.plot(bce_e, lw=1.0)\n",
    "ax.set_title(\"Epoch cross-entropy\")\n",
    "ax.set_xlabel(\"Epoch\")\n",
    "ax.set_ylabel(\"Cross-entropy\")\n",
    "fig.savefig(os.path.join(model_dir, \"epoch-bce\"))\n",
    "# plt.close(fig)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(Const.FIG_SIZE))\n",
    "ax.plot(recall_e * 100, lw=1.0)\n",
    "ax.set_title(\"Epoch recall\")\n",
    "ax.set_xlabel(\"Epoch\")\n",
    "ax.set_ylabel(\"Recall [\\%]\")\n",
    "# ax.set_ylim([0, 100])\n",
    "fig.savefig(os.path.join(model_dir, \"epoch-recall\"))\n",
    "# plt.close(fig)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(Const.FIG_SIZE))\n",
    "ax.plot(precision_e * 100, lw=1.0)\n",
    "ax.set_title(\"Epoch precision\")\n",
    "ax.set_xlabel(\"Epoch\")\n",
    "ax.set_ylabel(\"Precision [\\%]\")\n",
    "# ax.set_ylim([0, 100])\n",
    "fig.savefig(os.path.join(model_dir, \"epoch-precision\"))\n",
    "# plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sonnet as snt\n",
    "from lib.tf_utils import print_variables\n",
    "\n",
    "n_nodes = 14\n",
    "\n",
    "class OutputModel(snt.Module):\n",
    "    def __init__(self, n_edge_features, n_node_features, n_edges, n_nodes):\n",
    "        super(OutputModel, self).__init__(name=\"output_model\")\n",
    "        \n",
    "        self.n_edge_features = n_edge_features\n",
    "        self.n_node_features = n_node_features\n",
    "        self.n_edges = n_edges\n",
    "        self.n_nodes = n_nodes\n",
    "        \n",
    "        self.layer_concat = tf.keras.layers.Concatenate(\n",
    "            axis=-1, name=\"layer_concat\", dtype=tf.float64\n",
    "        )\n",
    "        \n",
    "        self.layer_linear = snt.Linear(output_size=1, with_bias=True, name=\"layer_linear\")\n",
    "        \n",
    "    def __call__(self, input_graphs):\n",
    "        edges = tf.reshape(input_graphs.edges, shape=[-1, 2 * self.n_edges, self.n_edge_features])\n",
    "        edges = tf.math.reduce_max(edges, axis=1)\n",
    "        print(edges.shape) \n",
    "        \n",
    "        nodes = tf.reshape(input_graphs.nodes, shape=[-1, self.n_nodes, self.n_node_features])\n",
    "        nodes = tf.math.reduce_max(nodes, axis=1)\n",
    "        print(nodes.shape)\n",
    "        \n",
    "        x = self.layer_concat([nodes, edges])\n",
    "        print(x.shape)\n",
    "        \n",
    "        x = self.layer_linear(x)\n",
    "        print(x.shape)\n",
    "        \n",
    "        x = tf.reshape(x, shape=[-1])\n",
    "        print(x.shape)\n",
    "        return x\n",
    "        \n",
    "output_model = OutputModel(n_edge_features=graph_dims[\"n_edge_features\"],\n",
    "                           n_node_features=graph_dims[\"n_node_features\"],\n",
    "                           n_edges=n_edges,\n",
    "                          n_nodes=n_nodes)\n",
    "\n",
    "print(tf.reshape(model.graph_network(g_stacked).edges, shape=[-1, 2 * n_edges, graph_dims[\"n_edge_features\"]]).shape)\n",
    "print(tf.reshape(model.graph_network(g_stacked).nodes, shape=[-1, n_nodes, graph_dims[\"n_node_features\"]]).shape)\n",
    "\n",
    "logits = output_model(model.graph_network(g_stacked))\n",
    "\n",
    "print(output_model.submodules)\n",
    "print_variables(output_model.trainable_variables)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Chronic datasets.\n",
    "\"\"\"\n",
    "\n",
    "dataset_by_chronic = dict()\n",
    "for chronic_idx, chronic_len in zip(collector.chronic_ids, collector.chronic_lengths):\n",
    "    dataset_by_chronic[chronic_idx] = dataset.take(chronic_len)\n",
    "    print(tf.data.experimental.cardinality(dataset_by_chronic[chronic_idx]))\n",
    "    dataset = dataset.skip(chronic_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
