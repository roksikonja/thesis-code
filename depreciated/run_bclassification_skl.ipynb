{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "L2RPN_2019_ART (dc)\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "                                        Loading Experience\n",
      "--------------------------------------------------------------------------------\n",
      "    - Loading chronics:                 ./results/performance-aug/l2rpn_2019_art-dc/agent-mip-chronic-****\n",
      "    - Number of chronics:               67\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "from scipy.stats import randint as sp_randint\n",
    "from scipy.stats import uniform\n",
    "from sklearn.ensemble import GradientBoostingClassifier, BaggingClassifier\n",
    "from sklearn.ensemble import (\n",
    "    RandomForestClassifier,\n",
    "    ExtraTreesClassifier,\n",
    "    AdaBoostClassifier,\n",
    ")\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "from bclassification.utils_fc import (\n",
    "    print_dataset,\n",
    "    obs_to_vect_with_tc,\n",
    "    obs_vects_to_vect,\n",
    ")\n",
    "from bclassification.utils_skl import train, train_model\n",
    "from experience import load_experience\n",
    "from lib.action_space import is_do_nothing_action\n",
    "from lib.constants import Constants as Const\n",
    "from lib.data_utils import make_dir, env_pf, extract_target_windows, moving_window\n",
    "from lib.dc_opf import TopologyConverter\n",
    "from lib.visualizer import Visualizer, pprint\n",
    "\n",
    "Visualizer()\n",
    "\n",
    "# experience_dir = make_dir(os.path.join(Const.EXPERIENCE_DIR, \"data-aug\"))\n",
    "experience_dir = make_dir(os.path.join(Const.RESULTS_DIR, \"performance-aug\"))\n",
    "results_dir = make_dir(os.path.join(Const.RESULTS_DIR, \"bc-fc\"))\n",
    "\n",
    "agent_name = \"agent-mip\"\n",
    "case_name = \"l2rpn_2019_art\"\n",
    "env_dc = True\n",
    "verbose = False\n",
    "\n",
    "case_results_dir = make_dir(os.path.join(results_dir, f\"{case_name}-{env_pf(env_dc)}\"))\n",
    "case, collector = load_experience(case_name, agent_name, experience_dir, env_dc=env_dc)\n",
    "\n",
    "pprint(\"    - Number of chronics:\", len(collector.chronic_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Parameters\n",
    "\"\"\"\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "random_seed = 1\n",
    "\n",
    "model_type = \"skl\"\n",
    "\n",
    "n_window_targets = 0\n",
    "n_window_history = 1\n",
    "\n",
    "test_frac = 0.10\n",
    "downsampling_rate = 0.05\n",
    "\n",
    "param_dist = {\n",
    "    \"svm\": {\n",
    "        \"param_dist\": {\n",
    "            \"C\": uniform(0.2, 1.2),\n",
    "        },\n",
    "        \"cls\": SVC(class_weight='balanced',random_state=random_seed, probability=True),\n",
    "    },\n",
    "    \"et\": {\n",
    "        \"param_dist\": {\n",
    "            \"criterion\": [\"gini\", \"entropy\"],\n",
    "            \"max_depth\": sp_randint(1, 3),\n",
    "            \"n_estimators\": sp_randint(10, 20),\n",
    "        },\n",
    "        \"cls\": ExtraTreesClassifier(\n",
    "            random_state=random_seed, n_jobs=-1, class_weight=\"balanced\"\n",
    "        ),\n",
    "    },\n",
    "    \"ada\": {\n",
    "        \"param_dist\": {\"n_estimators\": sp_randint(10, 20)},\n",
    "        \"cls\": AdaBoostClassifier(learning_rate=0.1, random_state=random_seed),\n",
    "    },\n",
    "    \"rf\": {\n",
    "        \"param_dist\": {\n",
    "            \"criterion\": [\"gini\", \"entropy\"],\n",
    "            \"max_depth\": sp_randint(1, 3),\n",
    "            \"n_estimators\": sp_randint(10, 20),\n",
    "        },\n",
    "        \"cls\": RandomForestClassifier(\n",
    "            class_weight=\"balanced\", random_state=random_seed, n_jobs=-1\n",
    "        ),\n",
    "    },\n",
    "    \"dtc\": {\n",
    "        \"param_dist\": {\n",
    "            \"max_depth\": sp_randint(1, 3),\n",
    "            \"min_samples_split\": uniform(0, 1.0),\n",
    "        },\n",
    "        \"cls\": DecisionTreeClassifier(\n",
    "            class_weight=\"balanced\", random_state=random_seed\n",
    "        ),\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Datasets\n",
    "\"\"\"\n",
    "from lib.data_utils import indices_to_hot\n",
    "from lib.data_utils import extract_history_windows\n",
    "\n",
    "np.random.seed(random_seed)\n",
    "\n",
    "labels = []\n",
    "mask_targets = []\n",
    "Y_all = []\n",
    "X_all = []\n",
    "\n",
    "obs_to_vect = obs_to_vect_with_tc(TopologyConverter(case.env))\n",
    "\n",
    "for chronic_idx, chronic_data in collector.data.items():\n",
    "    chronic_obses = chronic_data[\"obses\"][:-1]\n",
    "    chronic_labels = is_do_nothing_action(chronic_data[\"actions\"], case.env, dtype=np.bool)\n",
    "        \n",
    "    mask_positives = extract_history_windows(chronic_labels, n_window=n_window_targets)\n",
    "    mask_negatives = np.logical_and(\n",
    "        np.random.binomial(1, downsampling_rate, len(chronic_labels)).astype(np.bool), \n",
    "        ~mask_positives\n",
    "    )\n",
    "    chronic_mask_targets = np.logical_or(chronic_labels, mask_negatives)\n",
    "    \n",
    "    # Observation history features\n",
    "    chronic_X_obses = moving_window(\n",
    "        chronic_obses,\n",
    "        n_window=n_window_history,\n",
    "        process_fn=obs_to_vect,\n",
    "        combine_fn=obs_vects_to_vect,\n",
    "        padding=np.zeros_like(obs_to_vect(chronic_obses[0])),\n",
    "    )\n",
    "\n",
    "    # Action history features\n",
    "    chronic_actions = np.roll(chronic_labels, 1).astype(np.float)\n",
    "    chronic_actions[0] = 0.0\n",
    "\n",
    "    chronic_X_actions = moving_window(\n",
    "            chronic_actions,\n",
    "            n_window=n_window_history,\n",
    "            process_fn=lambda x: 10.0 * indices_to_hot([int(x)], length=2, dtype=np.float),\n",
    "            combine_fn=lambda x: np.concatenate(x),\n",
    "            padding=np.zeros((2, )),\n",
    "    )\n",
    "\n",
    "    chronic_X = np.hstack((chronic_X_obses, chronic_X_actions))\n",
    "\n",
    "    labels.extend(chronic_labels)\n",
    "    mask_targets.extend(chronic_mask_targets)\n",
    "    X_all.extend(chronic_X)\n",
    "    Y_all.extend(chronic_labels)\n",
    "    \n",
    "labels = np.array(labels)\n",
    "mask_targets = np.array(mask_targets)\n",
    "X_all = np.vstack(X_all).astype(np.float)\n",
    "Y_all = np.array(Y_all).astype(np.int)\n",
    "\n",
    "X = X_all[mask_targets, :]\n",
    "Y = Y_all[mask_targets]\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "    X, Y, test_size=test_frac, random_state=random_seed\n",
    ")\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(\n",
    "    X_train, Y_train, test_size=test_frac, random_state=random_seed\n",
    ")\n",
    "\n",
    "pprint(\n",
    "    \"    - Labels:\",\n",
    "    f\"{labels.sum()}/{labels.size}\",\n",
    "    \"{:.2f} %\".format(100 * labels.mean()),\n",
    ")\n",
    "\n",
    "print_dataset(X_all, Y_all, \"All data\")\n",
    "print_dataset(X, Y, \"Data\")\n",
    "print_dataset(X_train, Y_train, \"Train\")\n",
    "print_dataset(X_val, Y_val, \"Validation\")\n",
    "print_dataset(X_test, Y_test, \"Test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(\n",
    "    train_data=(X_train, Y_train),\n",
    "    test_data=(X_test, Y_test),\n",
    "    scaling=True,\n",
    "    power_scaling=True,\n",
    "    random_search=(True, param_dist, 10,),\n",
    "    cross_validation=(True, 5, \"balanced_accuracy\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    matthews_corrcoef,\n",
    "    precision_recall_fscore_support,\n",
    ")\n",
    "from sklearn.model_selection import RandomizedSearchCV, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler, PowerTransformer\n",
    "\n",
    "from lib.visualizer import pprint\n",
    "\n",
    "def describe_dataset(x, y, name):\n",
    "    labels = np.unique(y)\n",
    "    count = [np.count_nonzero(y == c) for c in labels]\n",
    "    weights = [c / sum(count) for c in count]\n",
    "\n",
    "    pprint(f\"    - {name}\", \"X\" + str(x.shape), \"Y\" + str(y.shape))\n",
    "    pprint(\"        - Labels:\", str(labels))\n",
    "    pprint(\"        - Count:\", str(count))\n",
    "    pprint(\"        - Weights:\", str(weights))\n",
    "    return labels, count, weights\n",
    "\n",
    "\n",
    "def train_model(\n",
    "        train_data,\n",
    "        test_data,\n",
    "        model,\n",
    "        scaling=True,\n",
    "        power_scaling=False,\n",
    "        random_search=(False,),\n",
    "        cross_validation=(False, 5, \"balanced_accuracy\"),\n",
    "):\n",
    "    x_train, y_train = train_data\n",
    "    x_test, y_test = test_data\n",
    "\n",
    "    # Load data\n",
    "    describe_dataset(x_train, y_train, \"Train\")\n",
    "    describe_dataset(x_test, y_test, \"Test\")\n",
    "\n",
    "    # Feature scaling\n",
    "    if scaling:\n",
    "        scaler = StandardScaler()\n",
    "        scaler.fit(x_train)\n",
    "        x_train = scaler.transform(x_train)\n",
    "        x_test = scaler.transform(x_test)\n",
    "\n",
    "    if power_scaling:\n",
    "        ptransformer = PowerTransformer()\n",
    "        ptransformer.fit(x_train)\n",
    "        x_train = ptransformer.transform(x_train)\n",
    "        x_test = ptransformer.transform(x_test)\n",
    "\n",
    "    # Training\n",
    "    best_est = None\n",
    "    if random_search[0]:\n",
    "        rs = RandomizedSearchCV(\n",
    "            model,\n",
    "            param_distributions=random_search[1],\n",
    "            n_iter=random_search[2],\n",
    "            cv=5,\n",
    "            n_jobs=-1,\n",
    "            random_state=0,\n",
    "            scoring=random_search[3],\n",
    "        )\n",
    "        rs.fit(x_train, y_train.ravel())\n",
    "        best_est = rs.best_estimator_\n",
    "        pprint(f\"    - Random Search best:\", str(rs.best_estimator_))\n",
    "\n",
    "    if best_est:\n",
    "        model = best_est\n",
    "\n",
    "    model.fit(x_train, y_train.ravel())\n",
    "\n",
    "    y_train_pred = model.predict(x_train)\n",
    "    y_test_pred = model.predict(x_test)\n",
    "\n",
    "    pprint(\"    - MCC:\", \"\")\n",
    "    pprint(\"        - Train:\", matthews_corrcoef(y_train, y_train_pred))\n",
    "    pprint(\"        - Test:\", matthews_corrcoef(y_test, y_test_pred))\n",
    "\n",
    "    if cross_validation[0]:\n",
    "        pprint(\"    - CV:\", f\"{cross_validation[1]}-fold\")\n",
    "        for metric in [\"f1\", \"accuracy\", \"balanced_accuracy\", \"recall\", \"precision\"]:\n",
    "            score = cross_val_score(\n",
    "                model,\n",
    "                np.vstack((x_train, x_test)),\n",
    "                np.hstack((y_train, y_test)).ravel(),\n",
    "                cv=cross_validation[1],\n",
    "                scoring=metric,\n",
    "                n_jobs=-1,\n",
    "            )\n",
    "            pprint(\"    - Metric:\", metric)\n",
    "            pprint(\"        - Mean:\", score.mean())\n",
    "            pprint(\"        - Std:\", score.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model:                                  SVM\n",
      "    - Train                             X(20208, 496)\tY(20208,)\n",
      "        - Labels:                       [0 1]\n",
      "        - Count:                        [16837, 3371]\n",
      "        - Weights:                      [0.8331848772763262, 0.1668151227236738]\n",
      "    - Test                              X(2495, 496)\tY(2495,)\n",
      "        - Labels:                       [0 1]\n",
      "        - Count:                        [2054, 441]\n",
      "        - Weights:                      [0.823246492985972, 0.17675350701402806]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "divide by zero encountered in log\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    - Random Search best:               SVC(C=1.0582272396469035, class_weight='balanced', probability=True,\n",
      "    random_state=1)\n",
      "    - MCC:                              \n",
      "        - Train:                        0.5319812482799416\n",
      "        - Test:                         0.444545988772654\n",
      "    - CV:                               5-fold\n",
      "    - Metric:                           f1\n",
      "        - Mean:                         0.5392353378630034\n",
      "        - Std:                          0.007308380237401398\n",
      "    - Metric:                           accuracy\n",
      "        - Mean:                         0.7703826031449145\n",
      "        - Std:                          0.007437372312604266\n"
     ]
    }
   ],
   "source": [
    "for model_name, model_params in param_dist.items():\n",
    "    pprint(\"Model:\", model_name.upper())\n",
    "    train_model(\n",
    "        train_data=(X_train, Y_train),\n",
    "        test_data=(X_test, Y_test),\n",
    "        model=model_params[\"cls\"],\n",
    "        scaling=True,\n",
    "        power_scaling=True,\n",
    "        random_search=(True, model_params[\"param_dist\"], 5, \"f1\"),\n",
    "        cross_validation=(True, 5),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_name, model_params in param_dist.items():\n",
    "    pprint(\"Model:\", model_name.upper())\n",
    "    if model_name == \"svm:\":\n",
    "        continue\n",
    "        \n",
    "    train_model(\n",
    "        train_data=(X_train, Y_train),\n",
    "        test_data=(X_test, Y_test),\n",
    "        model=model_params[\"cls\"],\n",
    "        scaling=True,\n",
    "        power_scaling=True,\n",
    "        random_search=(True, model_params[\"param_dist\"], 5, \"f1\"),\n",
    "        cross_validation=(True, 5),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
