{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "L2RPN_2019_ART (dc)\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "                                        Loading Experience\n",
      "--------------------------------------------------------------------------------\n",
      "    - Loading chronics:                 ./experience/data-aug-sample/l2rpn_2019_art-dc/agent-mip-chronic-****\n",
      "    - Number of loaded chronics:        10\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "import os\n",
    "import json\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from experience import load_experience\n",
    "from bclassification.utils_base import (\n",
    "    print_class_weights,\n",
    "    compute_weight_bias,\n",
    "    print_dataset,\n",
    "    plot_metrics,\n",
    "    plot_cm,\n",
    "    plot_roc,\n",
    "    describe_results,\n",
    ")\n",
    "from bclassification.utils_fcn import create_dataset\n",
    "from bclassification.dnn import load_dnn\n",
    "from lib.constants import Constants as Const\n",
    "from lib.data_utils import (\n",
    "    make_dir,\n",
    "    env_pf,\n",
    "    create_results_dir,\n",
    "    save_dict_to_file,\n",
    ")\n",
    "from lib.tf_utils import (\n",
    "    print_variables,\n",
    "    ResidulaFCBlock,\n",
    "    MatthewsCorrelationCoefficient,\n",
    ")\n",
    "from lib.visualizer import Visualizer, pprint\n",
    "\n",
    "\n",
    "experience_dir = make_dir(os.path.join(Const.EXPERIENCE_DIR, \"data-aug-sample\"))\n",
    "results_dir = make_dir(os.path.join(Const.RESULTS_DIR, \"paper\"))\n",
    "\n",
    "agent_name = \"agent-mip\"\n",
    "case_name = \"l2rpn_2019_art\"\n",
    "env_dc = True\n",
    "verbose = False\n",
    "\n",
    "case_results_dir = make_dir(os.path.join(results_dir, f\"{case_name}-{env_pf(env_dc)}\"))\n",
    "case, collector = load_experience(case_name, agent_name, experience_dir, env_dc=env_dc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Parameters\n",
    "\"\"\"\n",
    "\n",
    "random_seed = 1\n",
    "\n",
    "input_mode = \"structured\"\n",
    "label_mode = \"dn\"\n",
    "\n",
    "n_window_targets = 12  # 0 or 12\n",
    "n_window_history = 0\n",
    "downsampling_rate = 0.10\n",
    "n_window_forecasts = 1\n",
    "use_actions = True\n",
    "feature_scaling = True\n",
    "batch_normalization = False\n",
    "\n",
    "val_frac = 0.10\n",
    "test_frac = 0.10\n",
    "\n",
    "# Model\n",
    "model_type = \"res\"  # \"fc\" or \"res\"\n",
    "dropout_rate = 0\n",
    "l1_reg = 1e-7\n",
    "l2_reg = 0\n",
    "n_hidden = 512\n",
    "n_hidden_layers = 4\n",
    "threshold = 0.50\n",
    "pos_scaling = 1\n",
    "\n",
    "# Training\n",
    "learning_rate = 1e-5\n",
    "n_batch = 512\n",
    "n_epochs = 250"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    - Input structure:                  structured\n",
      "    - Label structure:                  dn\n",
      "    - Labels:                           655/60864\t1.08 %\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "    Dataset\n",
    "\"\"\"\n",
    "\n",
    "_, _, mask_targets, X_all, Y_all, p_scaling_std = create_dataset(\n",
    "    case,\n",
    "    collector,\n",
    "    input_mode=input_mode,\n",
    "    label_mode=label_mode,\n",
    "    n_window_history=n_window_history,\n",
    "    n_window_targets=n_window_targets,\n",
    "    downsampling_rate=downsampling_rate,\n",
    "    n_window_forecasts=n_window_forecasts,\n",
    "    use_actions=use_actions,\n",
    "    feature_scaling=feature_scaling,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chronics:                               [0, 1, 2, 3, 12, 18, 19, 104, 105, 106]\n",
      "intersection                            0\n",
      "union                                   60864\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "    Dataset Split\n",
    "\"\"\"\n",
    "\n",
    "np.random.seed(random_seed)\n",
    "tf.random.set_seed(random_seed)\n",
    "\n",
    "pprint(\"chronics:\", collector.chronic_ids)\n",
    "\n",
    "chronics_train, chronics_val = train_test_split(\n",
    "     collector.chronic_ids, test_size=val_frac, random_state=random_seed\n",
    ")\n",
    "\n",
    "chronics_train, chronics_test = train_test_split(\n",
    "     chronics_train, test_size=test_frac / (1 - val_frac), random_state=random_seed\n",
    ")\n",
    "\n",
    "chronics_train.sort()\n",
    "chronics_val.sort()\n",
    "chronics_test.sort()\n",
    "\n",
    "train_ids = np.zeros_like(Y_all, dtype=np.bool)\n",
    "val_ids = np.zeros_like(Y_all, dtype=np.bool)\n",
    "test_ids = np.zeros_like(Y_all, dtype=np.bool)\n",
    "\n",
    "start = 0\n",
    "for chronic_id, chronic_len in zip(collector.chronic_ids, collector.chronic_lengths):\n",
    "    end = start + chronic_len\n",
    "    \n",
    "    if chronic_id in chronics_train:\n",
    "        train_ids[start:end] = True\n",
    "    elif chronic_id in chronics_val:\n",
    "        val_ids[start:end] = True\n",
    "    elif chronic_id in chronics_test:\n",
    "        test_ids[start:end] = True\n",
    "    start = end\n",
    "\n",
    "# test\n",
    "pprint(\"intersection\", np.sum(train_ids &  val_ids &  test_ids))\n",
    "pprint(\"union\", np.sum(train_ids |  val_ids |  test_ids))\n",
    "\n",
    "mask_train = mask_targets & train_ids\n",
    "mask_val = mask_targets & val_ids\n",
    "mask_test = mask_targets & test_ids\n",
    "\n",
    "X_train, Y_train = X_all[mask_train, :], Y_all[mask_train]\n",
    "X_val, Y_val = X_all[mask_val, :], Y_all[mask_val]\n",
    "X_test, Y_test = X_all[mask_test, :], Y_all[mask_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture cap --no-stderr\n",
    "class_weight, initial_bias = compute_weight_bias(Y_all[mask_targets])\n",
    "initial_bias = 0\n",
    "class_weight[1] = class_weight[1] * pos_scaling \n",
    "\n",
    "print_dataset(X_all, Y_all, \"All data\")\n",
    "print_dataset(X_all[mask_targets], Y_all[mask_targets], \"Data\")\n",
    "print_dataset(X_train, Y_train, \"Train\")\n",
    "print_dataset(X_val, Y_val, \"Validation\")\n",
    "print_dataset(X_test, Y_test, \"Test\")\n",
    "\n",
    "print_class_weights(class_weight)\n",
    "pprint(\"Initial bias:\", \"{:.4f}\".format(float(initial_bias)))\n",
    "\n",
    "model_dir = create_results_dir(case_results_dir, model_name=model_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model directory:                        ./results\\paper\\l2rpn_2019_art-dc\\2021-02-16_20-17-13_res\n",
      "Restoring checkpoint from:              ./results\\paper\\l2rpn_2019_art-dc\\2021-02-16_20-17-13_res\\ckpts\\ckpt-2\n",
      "{\n",
      "  \"batch_normalization\": false,\n",
      "  \"downsampling_rate\": 0.1,\n",
      "  \"dropout_rate\": 0,\n",
      "  \"feature_scaling\": true,\n",
      "  \"input_dim\": 214,\n",
      "  \"input_mode\": \"structured\",\n",
      "  \"l1_reg\": 1e-07,\n",
      "  \"l2_reg\": 0,\n",
      "  \"label_mode\": \"dn\",\n",
      "  \"learning_rate\": 1e-05,\n",
      "  \"model_type\": \"res\",\n",
      "  \"n_batch\": 512,\n",
      "  \"n_epochs\": 250,\n",
      "  \"n_hidden\": 512,\n",
      "  \"n_hidden_layers\": 4,\n",
      "  \"n_window_forecasts\": 1,\n",
      "  \"n_window_history\": 0,\n",
      "  \"n_window_targets\": 12,\n",
      "  \"p_scaling_std\": 55.24142991792492,\n",
      "  \"pos_scaling\": 1,\n",
      "  \"random_seed\": 1,\n",
      "  \"threshold\": 0.5,\n",
      "  \"use_actions\": true,\n",
      "  \"val_frac\": 0.1\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "    Model\n",
    "\"\"\"\n",
    "\n",
    "metrics = [\n",
    "    tf.keras.metrics.TruePositives(thresholds=threshold, name=\"tp\"),\n",
    "    tf.keras.metrics.FalsePositives(thresholds=threshold, name=\"fp\"),\n",
    "    tf.keras.metrics.TrueNegatives(thresholds=threshold, name=\"tn\"),\n",
    "    tf.keras.metrics.FalseNegatives(thresholds=threshold, name=\"fn\"),\n",
    "    tf.keras.metrics.BinaryAccuracy(threshold=threshold, name=\"accuracy\"),\n",
    "    tf.keras.metrics.Precision(thresholds=threshold, name=\"precision\"),\n",
    "    tf.keras.metrics.Recall(thresholds=threshold, name=\"recall\"),\n",
    "    MatthewsCorrelationCoefficient(threshold=threshold, name=\"mcc\"),\n",
    "    tf.keras.metrics.AUC(name=\"auc\"),\n",
    "]\n",
    "\n",
    "input_dim = X_train.shape[-1]\n",
    "\n",
    "tf.random.set_seed(random_seed)\n",
    "\n",
    "\n",
    "model = load_dnn(\n",
    "    input_dim,\n",
    "    n_hidden_layers,\n",
    "    n_hidden,\n",
    "    model_type,\n",
    "    l1_reg,\n",
    "    l2_reg,\n",
    "    dropout_rate,\n",
    "    batch_normalization,\n",
    "    initial_bias,\n",
    ")\n",
    "\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(lr=learning_rate),\n",
    "    loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "    metrics=metrics,\n",
    ")\n",
    "\n",
    "checkpoint_path = os.path.join(model_dir, \"ckpts\")\n",
    "ckpt = tf.train.Checkpoint(model=model, optimizer=model.optimizer)\n",
    "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)\n",
    "\n",
    "pprint(\"Model directory:\", model_dir)\n",
    "\n",
    "if ckpt_manager.latest_checkpoint:\n",
    "    ckpt.restore(ckpt_manager.latest_checkpoint)\n",
    "    pprint(\"Restoring checkpoint from:\", ckpt_manager.latest_checkpoint)\n",
    "    \n",
    "param_dict = {\n",
    "        \"random_seed\": random_seed,\n",
    "        \"input_mode\": input_mode,\n",
    "        \"label_mode\": label_mode,\n",
    "        \"val_frac\": val_frac,\n",
    "        \"downsampling_rate\": downsampling_rate,\n",
    "        \"n_window_targets\": n_window_targets,\n",
    "        \"n_window_history\": n_window_history,\n",
    "        \"n_window_forecasts\": n_window_forecasts,\n",
    "        \"use_actions\": use_actions,\n",
    "        \"feature_scaling\": feature_scaling,\n",
    "        \"p_scaling_std\": p_scaling_std,\n",
    "        \"model_type\": model_type,\n",
    "        \"input_dim\": input_dim,\n",
    "        \"dropout_rate\": dropout_rate,\n",
    "        \"l1_reg\": l1_reg,\n",
    "        \"l2_reg\": l2_reg,\n",
    "        \"n_hidden\": n_hidden,\n",
    "        \"n_hidden_layers\": n_hidden_layers,\n",
    "        \"learning_rate\": learning_rate,\n",
    "        \"n_batch\": n_batch,\n",
    "        \"n_epochs\": n_epochs,\n",
    "        \"threshold\": threshold,\n",
    "        \"pos_scaling\": pos_scaling,\n",
    "        \"batch_normalization\": batch_normalization,\n",
    "}\n",
    "print(json.dumps(param_dict, indent=2, sort_keys=True))\n",
    "save_dict_to_file(\n",
    "    param_dict,\n",
    "    os.path.join(model_dir, \"params.txt\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/250\n",
      "Epoch 2/250\n",
      "Epoch 3/250\n",
      "Epoch 4/250\n",
      "Epoch 5/250\n",
      "Epoch 6/250\n",
      "Epoch 7/250\n",
      "Epoch 8/250\n",
      "Epoch 9/250\n",
      "Epoch 10/250\n",
      "Epoch 11/250\n",
      "Epoch 12/250\n",
      "Epoch 13/250\n",
      "Epoch 14/250\n",
      "Epoch 15/250\n",
      "Epoch 16/250\n",
      "Epoch 17/250\n",
      "Epoch 18/250\n",
      "Epoch 19/250\n",
      "Epoch 20/250\n",
      "Epoch 21/250\n",
      "Epoch 22/250\n",
      "Epoch 23/250\n",
      "Epoch 24/250\n",
      "Epoch 25/250\n",
      "Epoch 26/250\n",
      "Epoch 27/250\n",
      "Epoch 28/250\n",
      "Epoch 29/250\n",
      "Epoch 30/250\n",
      "Epoch 31/250\n",
      "Epoch 32/250\n",
      "Epoch 33/250\n",
      "Epoch 34/250\n",
      "Epoch 35/250\n",
      "Epoch 36/250\n",
      "Epoch 37/250\n",
      "Epoch 38/250\n",
      "Epoch 39/250\n",
      "Epoch 40/250\n",
      "Epoch 41/250\n",
      "Epoch 42/250\n",
      "Epoch 43/250\n",
      "Epoch 44/250\n",
      "Epoch 45/250\n",
      "Epoch 46/250\n",
      "Epoch 47/250\n",
      "Epoch 48/250\n",
      "Epoch 49/250\n",
      "Epoch 50/250\n",
      "Epoch 51/250\n",
      "Epoch 52/250\n",
      "Epoch 53/250\n",
      "Epoch 54/250\n",
      "Epoch 55/250\n",
      "Epoch 56/250\n",
      "Epoch 57/250\n",
      "Epoch 58/250\n",
      "Epoch 59/250\n",
      "Epoch 60/250\n",
      "Epoch 61/250\n",
      "Epoch 62/250\n",
      "Epoch 63/250\n",
      "Epoch 64/250\n",
      "Epoch 65/250\n",
      "Epoch 66/250\n",
      "Epoch 67/250\n",
      "Epoch 68/250\n",
      "Epoch 69/250\n",
      "Epoch 70/250\n",
      "Epoch 71/250\n",
      "Epoch 72/250\n",
      "Epoch 73/250\n",
      "Epoch 74/250\n",
      "Epoch 75/250\n",
      "Epoch 76/250\n",
      "Epoch 77/250\n",
      "Epoch 78/250\n",
      "Epoch 79/250\n",
      "Epoch 80/250\n",
      "Epoch 81/250\n",
      "Epoch 82/250\n",
      "Epoch 83/250\n",
      "Epoch 84/250\n",
      "Epoch 85/250\n",
      "Epoch 86/250\n",
      "Epoch 87/250\n",
      "Epoch 88/250\n",
      "Epoch 89/250\n",
      "Epoch 90/250\n",
      "Epoch 91/250\n",
      "Epoch 92/250\n",
      "Epoch 93/250\n",
      "Epoch 94/250\n",
      "Epoch 95/250\n",
      "Epoch 96/250\n",
      "Epoch 97/250\n",
      "Epoch 98/250\n",
      "Epoch 99/250\n",
      "Epoch 100/250\n",
      "Epoch 101/250\n",
      "Epoch 102/250\n",
      "Epoch 103/250\n",
      "Epoch 104/250\n",
      "Epoch 105/250\n",
      "Epoch 106/250\n",
      "Epoch 107/250\n",
      "Epoch 108/250\n",
      "Epoch 109/250\n",
      "Epoch 110/250\n",
      "Epoch 111/250\n",
      "Epoch 112/250\n",
      "Epoch 113/250\n",
      "Epoch 114/250\n",
      "Epoch 115/250\n",
      "Epoch 116/250\n",
      "Epoch 117/250\n",
      "Epoch 118/250\n",
      "Epoch 119/250\n",
      "Epoch 120/250\n",
      "Epoch 121/250\n",
      "Epoch 122/250\n",
      "Epoch 123/250\n",
      "Epoch 124/250\n",
      "Epoch 125/250\n",
      "Epoch 126/250\n",
      "Epoch 127/250\n",
      "Epoch 128/250\n",
      "Epoch 129/250\n",
      "Epoch 130/250\n",
      "Epoch 131/250\n",
      "Epoch 132/250\n",
      "Epoch 133/250\n",
      "Epoch 134/250\n",
      "Epoch 135/250\n",
      "Epoch 136/250\n",
      "Epoch 137/250\n",
      "Epoch 138/250\n",
      "Epoch 139/250\n",
      "Epoch 140/250\n",
      "Epoch 141/250\n",
      "Epoch 142/250\n",
      "Epoch 143/250\n",
      "Epoch 144/250\n",
      "Epoch 145/250\n",
      "Epoch 146/250\n",
      "Epoch 147/250\n",
      "Epoch 148/250\n",
      "Epoch 149/250\n",
      "Epoch 150/250\n",
      "Epoch 151/250\n",
      "Epoch 152/250\n",
      "Epoch 153/250\n",
      "Epoch 154/250\n",
      "Epoch 155/250\n",
      "Epoch 156/250\n",
      "Epoch 157/250\n",
      "Epoch 158/250\n",
      "Epoch 159/250\n",
      "Epoch 160/250\n",
      "Epoch 161/250\n",
      "Epoch 162/250\n",
      "Epoch 163/250\n",
      "Epoch 164/250\n",
      "Epoch 165/250\n",
      "Epoch 166/250\n",
      "Epoch 167/250\n",
      "Epoch 168/250\n",
      "Epoch 169/250\n",
      "Epoch 170/250\n",
      "Epoch 171/250\n",
      "Epoch 172/250\n",
      "Epoch 173/250\n",
      "Epoch 174/250\n",
      "Epoch 175/250\n",
      "Epoch 176/250\n",
      "Epoch 177/250\n",
      "Epoch 178/250\n",
      "Epoch 179/250\n",
      "Epoch 180/250\n",
      "Epoch 181/250\n",
      "Epoch 182/250\n",
      "Epoch 183/250\n",
      "Epoch 184/250\n",
      "Epoch 185/250\n",
      "Epoch 186/250\n",
      "Epoch 187/250\n",
      "Epoch 188/250\n",
      "Epoch 189/250\n",
      "Epoch 190/250\n",
      "Epoch 191/250\n",
      "Epoch 192/250\n",
      "Epoch 193/250\n",
      "Epoch 194/250\n",
      "Epoch 195/250\n",
      "Epoch 196/250\n",
      "Epoch 197/250\n",
      "Epoch 198/250\n",
      "Epoch 199/250\n",
      "Epoch 200/250\n",
      "Epoch 201/250\n",
      "Epoch 202/250\n",
      "Epoch 203/250\n",
      "Epoch 204/250\n",
      "Epoch 205/250\n",
      "Epoch 206/250\n",
      "Epoch 207/250\n",
      "Epoch 208/250\n",
      "Epoch 209/250\n",
      "Epoch 210/250\n",
      "Epoch 211/250\n",
      "Epoch 212/250\n",
      "Epoch 213/250\n",
      "Epoch 214/250\n",
      "Epoch 215/250\n",
      "Epoch 216/250\n",
      "Epoch 217/250\n",
      "Epoch 218/250\n",
      "Epoch 219/250\n",
      "Epoch 220/250\n",
      "Epoch 221/250\n",
      "Epoch 222/250\n",
      "Epoch 223/250\n",
      "Epoch 224/250\n",
      "Epoch 225/250\n",
      "Epoch 226/250\n",
      "Epoch 227/250\n",
      "Epoch 228/250\n",
      "Epoch 229/250\n",
      "Epoch 230/250\n",
      "Epoch 231/250\n",
      "Epoch 232/250\n",
      "Epoch 233/250\n",
      "Epoch 234/250\n",
      "Epoch 235/250\n",
      "Epoch 236/250\n",
      "Epoch 237/250\n",
      "Epoch 238/250\n",
      "Epoch 239/250\n",
      "Epoch 240/250\n",
      "Epoch 241/250\n",
      "Epoch 242/250\n",
      "Epoch 243/250\n",
      "Epoch 244/250\n",
      "Epoch 245/250\n",
      "Epoch 246/250\n",
      "Epoch 247/250\n",
      "Epoch 248/250\n",
      "Epoch 249/250\n",
      "Epoch 250/250\n",
      "    - Saving checkpoint to:             ./results\\paper\\l2rpn_2019_art-dc\\2021-02-16_20-17-13_res\\ckpts\\ckpt-2\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "    Training\n",
    "\"\"\"\n",
    "\n",
    "training = model.fit(\n",
    "    X_train,\n",
    "    Y_train,\n",
    "    epochs=n_epochs,\n",
    "    batch_size=n_batch,\n",
    "    class_weight=class_weight,\n",
    "    validation_data=(X_val, Y_val),\n",
    "    verbose=3,\n",
    ")\n",
    "\n",
    "ckpt_save_path = ckpt_manager.save()\n",
    "pprint(f\"    - Saving checkpoint to:\", ckpt_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model(X_val[0:1, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture cap --no-stderr\n",
    "\"\"\"\n",
    "    Results\n",
    "\"\"\"\n",
    "from sklearn.metrics import f1_score, matthews_corrcoef, confusion_matrix\n",
    "from lib.rewards import correct_predictions\n",
    "\n",
    "if ckpt_manager.latest_checkpoint:\n",
    "    ckpt.restore(ckpt_manager.latest_checkpoint)\n",
    "    pprint(\"Restoring checkpoint from:\", ckpt_manager.latest_checkpoint)\n",
    "\n",
    "print_variables(model.trainable_variables)\n",
    "plot_metrics(training, Y_train, Y_val, save_dir=model_dir)\n",
    "\n",
    "results_train = model.evaluate(X_train, Y_train, batch_size=n_batch, verbose=0)\n",
    "results_val = model.evaluate(X_val, Y_val, batch_size=n_batch, verbose=0)\n",
    "results_test = model.evaluate(X_test, Y_test, batch_size=n_batch, verbose=0)\n",
    "\n",
    "Y_train_pred = model.predict(X_train, batch_size=n_batch)\n",
    "Y_val_pred = model.predict(X_val, batch_size=n_batch)\n",
    "Y_test_pred = model.predict(X_test, batch_size=n_batch)\n",
    "\n",
    "describe_results(model.metrics_names, results_train, Y_train, name=\"Train\")\n",
    "pprint(\"    - F1:\", \"{:.4f}\".format(f1_score(Y_train, np.greater(Y_train_pred, threshold)), average=\"binary\"))\n",
    "describe_results(model.metrics_names, results_val, Y_val, name=\"Validation\")\n",
    "pprint(\"    - F1:\", \"{:.4f}\".format(f1_score(Y_val, np.greater(Y_val_pred, threshold)), average=\"binary\"))\n",
    "describe_results(model.metrics_names, results_test, Y_test, name=\"Test\")\n",
    "pprint(\"    - F1:\", \"{:.4f}\".format(f1_score(Y_test, np.greater(Y_test_pred, threshold)), average=\"binary\"))\n",
    "\n",
    "plot_roc(\n",
    "    [\n",
    "        (\"Training\", Y_train, Y_train_pred),\n",
    "        (\"Validation\", Y_val, Y_val_pred),\n",
    "        (\"Test\", Y_test, Y_test_pred),\n",
    "    ],\n",
    "    save_dir=model_dir,\n",
    ")\n",
    "\n",
    "Y_all_train_pred = np.greater(model.predict(X_all[train_ids, :], batch_size=n_batch), threshold) \n",
    "Y_all_val_pred = np.greater(model.predict(X_all[val_ids, :], batch_size=n_batch), threshold)\n",
    "Y_all_test_pred = np.greater(model.predict(X_all[test_ids, :], batch_size=n_batch), threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "def describe_falses(y, y_pred, dataset_name):\n",
    "    cm = confusion_matrix(y, y_pred)\n",
    "    fp = cm[0, 1]\n",
    "    fn = cm[1, 0]\n",
    "    \n",
    "    n = np.sum(np.equal(y, 0))\n",
    "    p = np.sum(np.equal(y, 1))\n",
    "    fpr = fp / n\n",
    "    fnr = fn / p\n",
    "    \n",
    "    mcc = matthews_corrcoef(y, y_pred)\n",
    "    \n",
    "#     pprint(\"    - Dataset\", dataset_name)\n",
    "#     pprint(\"        - FPR (C):\",\"{:<15}{:>8.2f}\".format(\"{}/{}\".format(int(fp), int(n)), fpr))\n",
    "#     pprint(\"        - FNR (C):\",\"{:<15}{:>8.2f}\".format(\"{}/{}\".format(int(fn), int(p)), fnr))\n",
    "#     pprint(\"        - MCC (C):\", \"{:.4f}\".format(mcc))\n",
    "    \n",
    "    return mcc, fpr, fnr\n",
    "\n",
    "\n",
    "row = [\"w_f \\\\ w_b\"] + [str(w_b) for w_b in np.arange(0, 37, 12)]\n",
    "mccs_train = [row]\n",
    "mccs_val = [row]\n",
    "mccs_test = [row]\n",
    "\n",
    "for w_f in np.arange(0, 37, 12):\n",
    "    row_train = [str(w_f)]\n",
    "    row_val = [str(w_f)]\n",
    "    row_test = [str(w_f)]\n",
    "    \n",
    "    for w_b in np.arange(0, 37, 12):\n",
    "#         Y_train_pred_c = correct_predictions(Y_all[train_ids], Y_all_train_pred, w_f=w_f, w_b=w_b)[mask_targets[train_ids]]\n",
    "#         res = describe_falses(Y_train, Y_train_pred_c, \"Train\")\n",
    "#         row_train.append(\"{:.3f}/{:.3f}/{:.3f}\".format(*res))\n",
    "        \n",
    "        Y_val_pred_c = correct_predictions(Y_all[val_ids], Y_all_val_pred, w_f=w_f, w_b=w_b)[mask_targets[val_ids]]\n",
    "        res = describe_falses(Y_val, Y_val_pred_c, \"Val\")\n",
    "        row_val.append(\"{:.3f}/{:.3f}/{:.3f}\".format(*res))\n",
    "        \n",
    "        Y_test_pred_c = correct_predictions(Y_all[test_ids], Y_all_test_pred, w_f=w_f, w_b=w_b)[mask_targets[test_ids]]\n",
    "        res = describe_falses(Y_test, Y_test_pred_c, \"Test\")\n",
    "        row_test.append(\"{:.3f}/{:.3f}/{:.3f}\".format(*res))\n",
    "\n",
    "    mccs_train.append(row_train)\n",
    "    mccs_val.append(row_val)\n",
    "    mccs_test.append(row_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"train\")\n",
    "print(pd.DataFrame(mccs_train).to_string(index=False))\n",
    "\n",
    "print(\"\\nval\")\n",
    "print(pd.DataFrame(mccs_val).to_string(index=False))\n",
    "\n",
    "print(\"\\ntest\")\n",
    "print(pd.DataFrame(mccs_test).to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap.show()\n",
    "\n",
    "with open(os.path.join(model_dir, \"log.txt\"), \"a\") as f:\n",
    "    f.write(cap.stdout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_experience_dir = make_dir(os.path.join(Const.EXPERIENCE_DIR, \"data-aug-sample\"))\n",
    "_, collector_sample = load_experience(case_name, agent_name, sample_experience_dir, env_dc=env_dc)\n",
    "\n",
    "_, _, _, X_all_sample, Y_all_sample = create_dataset(\n",
    "    case,\n",
    "    collector_sample,\n",
    "    input_mode=input_mode,\n",
    "    label_mode=label_mode,\n",
    "    n_window_history=n_window_history,\n",
    "    n_window_targets=n_window_targets,\n",
    "    downsampling_rate=downsampling_rate,\n",
    "    n_window_forecasts=n_window_forecasts,\n",
    "    use_actions=use_actions,\n",
    "    feature_scaling=feature_scaling,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use(\"seaborn-white\")\n",
    "plt.rcParams[\"font.family\"] = \"pcr\"\n",
    "plt.rcParams['axes.grid'] = False\n",
    "plt.rcParams['font.size'] = 20\n",
    "plt.rcParams['legend.fontsize'] = 20\n",
    "plt.rcParams['legend.title_fontsize'] = 20\n",
    "plt.rcParams['xtick.labelsize'] = 20\n",
    "plt.rcParams['ytick.labelsize'] = 20\n",
    "plt.rcParams['axes.labelsize'] = 20\n",
    "\n",
    "\n",
    "def plot_preds(t, y, y_pred, rhos, threshold, chronic_idx=None):\n",
    "    fig, ax = plt.subplots(figsize=(Const.FIG_SIZE[0] * 2, Const.FIG_SIZE[1]))\n",
    "\n",
    "    indices_pos = np.equal(y, 1)\n",
    "    ax.vlines(t[indices_pos], ymin=0.0, ymax=1.0, colors=\"0.0\", linestyle=\"-\", lw=2.0)\n",
    "    \n",
    "    indices_pos = np.greater(y_pred, threshold)\n",
    "    pos = np.ma.masked_where(np.greater_equal(y_pred, threshold), y_pred)\n",
    "    neg = np.ma.masked_where(np.less_equal(y_pred, threshold), y_pred)\n",
    "    \n",
    "    ax.plot(t, pos, color=\"0.8\", lw=2.0)\n",
    "    ax.plot(t, neg, color=\"0.5\", lw=2.0)\n",
    "    ax.plot(t, np.ones_like(t) * threshold, color=\"0\", linestyle=\"--\", lw=2.0)\n",
    "\n",
    "    ax.set_xlabel(r\"Time step $t$\")\n",
    "    ax.set_ylabel(r\"$P(y = 1 | \\, \\mathbf{x})$\")\n",
    "\n",
    "    if ax.get_xlim()[-1] > 1000:\n",
    "        ax.set_xlim(right=1010, left=-0.01 * 1000)\n",
    "    else:\n",
    "        ax.set_xlim(left=-0.01 * t.max(), right=1.01 * t.max())\n",
    "                \n",
    "    fig.tight_layout()\n",
    "    if model_dir and not isinstance(chronic_idx, type(None)):\n",
    "        fig.savefig(os.path.join(model_dir, \"test-y-step-{:04}\".format(chronic_idx)))\n",
    "        \n",
    "    if not isinstance(rhos, type(None)):\n",
    "        ax2 = ax.twinx()\n",
    "        ax2.set_ylabel(r\"$\\rho^\\mathrm{max}$\")\n",
    "        ax2.plot(t, rhos, label=r\"$y$\", color=\"0.1\", lw=2.0, linestyle=\"-.\")\n",
    "        ax2.set_ylim(*ax.get_ylim())\n",
    "        \n",
    "    fig.tight_layout()\n",
    "    if model_dir and not isinstance(chronic_idx, type(None)):\n",
    "        fig.savefig(os.path.join(model_dir, \"test-y-step-{:04}-rhos\".format(chronic_idx)))\n",
    "        \n",
    "start_idx = 0\n",
    "for chronic_idx, chronic_len in zip(collector_sample.chronic_ids, collector_sample.chronic_lengths):\n",
    "    X_chronic = X_all_sample[start_idx:(start_idx + chronic_len), :]\n",
    "    Y_chronic = Y_all_sample[start_idx:(start_idx + chronic_len)]\n",
    "    Y_chronic_pred = model.predict(X_chronic, batch_size=n_batch).flatten()\n",
    "    \n",
    "    results_chronic = model.evaluate(X_chronic, Y_chronic, batch_size=n_batch, verbose=0)\n",
    "    describe_results(model.metrics_names, results_chronic, Y_chronic, name=f\"Chronic {chronic_idx}\")\n",
    "\n",
    "    t = np.arange(chronic_len)\n",
    "    rhos = [np.max(obs.rho) for obs in collector_sample.data[chronic_idx][\"obses\"][:-1]]\n",
    "        \n",
    "    plot_preds(t, Y_chronic, Y_chronic_pred, rhos, threshold, chronic_idx)\n",
    "    \n",
    "    start_idx = start_idx + chronic_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
